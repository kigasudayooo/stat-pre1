import marimo

__generated_with = "0.11.20"
app = marimo.App(width="full")


@app.cell
def _():
    import marimo as mo
    import numpy as np
    return mo, np


@app.cell
def _(np):
    X = np.array([
        [2,2,3,1],
        [9,8,10,9],
        [8,3,2,7],
        [7,1,3,8],
        [2,9,8,2],
        [5,4,5,5],
    ])
    return (X,)


@app.cell
def _(np):
    def centering_matrix(X):
        """
        列単位の平均値を引いてセントリングする関数
        """
        n, d = X.shape

        I = np.eye(n)
        J = np.ones((n, n)) 
        C = I - (1/n) * J 

        X_centered = C @ X

        return X_centered
    return (centering_matrix,)


@app.cell
def _(X, centering_matrix):
    X_c = centering_matrix(X)
    return (X_c,)


@app.cell
def _(X, X_c):
    n ,_ = X.shape

    S = 1/(n-1) * X_c.T @ X_c
    S
    return S, n


@app.cell
def _(S, np):
    np.linalg.svd(S)
    return


@app.cell
def _(S, np):
    np.linalg.eig(S)
    return


@app.cell
def _(S, np):
    # 変数ごとの標準偏差を計算
    std_devs = np.sqrt(np.diag(S))
    std_devs
    return (std_devs,)


@app.cell
def _(np, std_devs):
    # 標準偏差の逆数で対角化した行列
    D_inv = np.diag(1/std_devs)
    D_inv
    return (D_inv,)


@app.cell
def _(D_inv, S):
    # 相関行列の計算: R = D^(-1) * S * D^(-1)
    R = D_inv @ S @ D_inv
    R
    return (R,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""

        ## 1. 主成分分析の基本概念

        主成分分析（PCA）とは、高次元データの重要な構造を低次元で表現するための手法である。本質的には、データの分散が最大になる方向（主成分）を順次見つけ出す手法である。

        データ行列を $X$ （サイズ $n \times d$、$n$個のサンプルと$d$個の特徴量）とするとき、まず各特徴量の平均が0になるようにセンタリングを行う：

        $$X_C = X - \bar{X}$$

        ここで $\bar{X}$ は各列（特徴量）の平均値からなる行列である。

        ## 2. 分散最大化としての主成分分析

        主成分分析の目的は、データの分散を最大化する方向を見つけることである。単位ベクトル $w$ （$|w|=1$）への射影を考えると、その射影データの分散は：

        $$\text{Var}(X_Cw) = \frac{1}{n-1}(X_Cw)^T(X_Cw) = \frac{1}{n-1}w^T X_C^T X_C w = w^T S w$$

        ここで $S = \frac{1}{n-1}X_C^T X_C$ はデータの共分散行列である。

        ## 3. データの方向への射影

        「データを方向 $w$ に射影する」とは、多次元データを特定の方向に沿って一次元の値に変換することである。

        射影の数学的表現：

        - 点 $x$ の方向 $w$ への射影値は内積 $x \cdot w$ で表される
        - 全データ点の射影値は行列とベクトルの積 $X_C w$ として計算できる

        射影は幾何学的には：

        1. 多次元空間内のデータ点から、方向 $w$ で定義される直線に垂線を下ろす
        2. 垂線の足が射影点となり、原点からの距離が射影値となる

        この射影の分散が大きいほど、その方向にデータが広がっていることを意味する。

        ## 4. ラグランジュ乗数法による分散最大化問題の解法

        ### ステップ1: 問題の定式化

        - 目的関数：$\max_{w} w^T S w$ （分散を最大化）
        - 制約条件：$w^T w = 1$ （$w$ は単位ベクトル）

        ### ステップ2: ラグランジュ関数の設定

        $$L(w, \lambda) = w^T S w - \lambda(w^T w - 1)$$

        ### ステップ3: 勾配を計算

        ラグランジュ関数の $w$ に関する勾配をゼロとおく： $$\nabla_w L(w, \lambda) = 0$$

        具体的な計算： $$\nabla_w L(w, \lambda) = \nabla_w(w^T S w) - \lambda \nabla_w(w^T w - 1)$$

        1. $\nabla_w(w^T S w) = 2Sw$（$S$が対称行列の場合）
        2. $\nabla_w(w^T w - 1) = 2w$

        これらを代入： $$\nabla_w L(w, \lambda) = 2Sw - \lambda \cdot 2w = 2(Sw - \lambda w) = 0$$

        整理して： $$Sw = \lambda w$$

        ### ステップ4: 解の意味

        この式は共分散行列 $S$ の固有値問題である：

        - $w$ は $S$ の固有ベクトル
        - $\lambda$ は対応する固有値

        ### ステップ5: 最適解の選択

        目的関数の値を計算： $$w^T S w = w^T (\lambda w) = \lambda w^T w = \lambda \cdot 1 = \lambda$$

        分散を最大化するには、最大の固有値 $\lambda_1$ に対応する固有ベクトル $w_1$ が最適解（第一主成分）となる。

        ### ステップ6: 第二主成分以降

        第二主成分は、第一主成分と直交（$w_1^T w_2 = 0$）しながら分散を最大化する方向である。共分散行列 $S$ は対称行列であるため、その固有ベクトルは互いに直交する。よって、固有値の大きさ順に対応する固有ベクトルが順番に主成分となる。

        ## 5. 特異値分解との関係

        データ行列 $X_C$ の特異値分解は： $$X_C = U\Sigma V^T$$

        ここで：

        - $U$：左特異ベクトル（$n \times n$の直交行列）
        - $\Sigma$：特異値を対角に持つ行列（$n \times d$）
        - $V$：右特異ベクトル（$d \times d$の直交行列）

        共分散行列 $S$ は特異値分解を用いて： $$S = \frac{1}{n-1}X_C^T X_C = \frac{1}{n-1}V\Sigma^T\Sigma V^T$$

        したがって：

        - $V$ の列は $S$ の固有ベクトル（主成分方向）
        - $S$ の固有値は $\frac{\sigma_i^2}{n-1}$（$\sigma_i$ は特異値）

        これにより、方向 $v_i$ への射影の分散は $\frac{\sigma_i^2}{n-1}$ となる。つまり特異値の大きさが、対応する方向の分散（情報量）を示している。

        ## 6. 座標系の変換としての解釈

        主成分分析は「座標系の回転」と解釈できる：

        - 元の座標系：標準的な特徴量空間
        - 新しい座標系：主成分方向を座標軸とする空間

        この変換では：

        - データの構造はそのままに、見る視点（座標系）が変わる
        - 新しい座標系では、軸が分散の大きさ順に並ぶ
        - 元の特徴量の線形結合として主成分が表現される

        このように、主成分分析は「重み付け」「方向への射影」「座標系の回転」という三つの見方が数学的に等価である手法である。

        ## 7. 適切な座標系を見つける他の手法

        統計学や機械学習には、様々な「最適な座標系を探す」手法が存在する：

        1. **線形判別分析（LDA）**：クラスの分離を最大化する座標系
        2. **カーネル法**：非線形の高次元空間への暗黙的な写像
        3. **多次元尺度法（MDS）**：距離関係を保存する低次元座標系
        4. **t-SNE/UMAP**：局所的な構造を保存する非線形次元削減
        5. **独立成分分析（ICA）**：統計的独立性を最大化する座標系
        6. **正準相関分析（CCA）**：異なるデータセット間の相関を最大化する共通座標系
        7. **Weighted Least Squares**：不均一分散に対応するためのスケーリング変換
        8. **Whitening変換**：特徴間の相関をなくし、等分散にする座標変換
        9. **スパース表現**：少数の基底ベクトルで効率的に表現できる座標系

        これらの手法はそれぞれ異なる最適性基準に基づいているが、共通して「データを適切な視点から見る」という目的を持つ。

        適切な座標系を見つけることの利点：

        - 問題の複雑さを軽減する
        - 計算効率を向上させる
        - データの解釈可能性を高める
        - ノイズを低減する

        これらの手法は、データ分析における「正しく見る」ための数学的枠組みを提供するものである。

        # 固有ベクトルが「新しい座標系」を表現する理由

        主成分分析で得られる固有ベクトルがなぜ「新しい座標系」を表すと言えるのかについて、数学的・幾何学的視点から説明する。

        ## 1. 座標系の基本的要件

        数学的に「座標系」とは、空間内の点の位置を一意に指定するための基底ベクトルの集合である。良い座標系の条件は：

        - **完全性**: 空間全体をカバーできる（任意の点が表現できる）
        - **線形独立性**: 各基底ベクトルが他の線形結合で表せない
        - 理想的には**直交性**: 基底ベクトル同士が直交している

        ## 2. 固有ベクトルの特性

        主成分分析で得られる共分散行列 $S$ の固有ベクトル ${w_1, w_2, ..., w_d}$ は以下の性質を持つ：

        1. **完全性**: $d$個の固有ベクトルは元の$d$次元空間全体をスパンする
        2. **直交性**: $S$は対称行列であるため、その固有ベクトルは互いに直交する（$w_i^T w_j = 0$ for $i \neq j$）
        3. **正規化**: 各固有ベクトルは単位長さ（$|w_i| = 1$）に正規化される

        これらの性質から、固有ベクトルの集合は「直交正規基底」を形成しており、これは座標系の基本的要件を完全に満たす。

        ## 3. 座標変換としての数学的表現

        元の座標系での点 $x$ を新しい座標系（主成分空間）での点 $z$ に変換する操作は以下のように表される：

        $$z = W^T x$$

        ここで $W = [w_1, w_2, ..., w_d]$ は固有ベクトルを列とする行列である。この変換は純粋な「回転変換」であり、データの形状自体は変わらない。これは $W$ が直交行列（$W^T W = I$）であることから保証される。

        逆変換も同様に：

        $$x = W z$$

        と表される。これは正確に座標変換の定義に合致する。

        ## 4. 幾何学的解釈

        固有値分解は、データの分散が最大になる方向を見つけるプロセスである。幾何学的には：

        1. 第一主成分 $w_1$ はデータの分散が最大となる方向を指す
        2. 第二主成分 $w_2$ は $w_1$ と直交しながら分散が次に大きい方向を指す
        3. 以下同様に、互いに直交する方向が分散の大きさ順に並ぶ

        この構造はまさに、データの広がり（分散）に合わせて最適化された「座標系」である。元のデータの座標を、この新しい軸に投影することで座標変換が行われる。

        ## 5. 具体例による説明

        2次元の楕円形に分布するデータを考える：

        - 第一主成分 $w_1$ は楕円の長軸方向を指す
        - 第二主成分 $w_2$ は楕円の短軸方向を指す

        この2つのベクトルは、元の $(x,y)$ 座標系とは異なる新しい座標系の基底を形成する。データ点の座標は、これらの基底ベクトルに沿った位置として再表現される。

        このように、主成分分析で得られる固有ベクトルは、データの構造に最適化された新しい座標系の基底そのものである。この座標系では、各軸がデータの分散（情報量）の大きさ順に配置されており、これがデータの本質的構造を理解するのに役立つ。
        """
    )
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
        # 特異値分解と固有値分解の関係

        ## 基本概念

        ### 固有値分解 (Eigendecomposition)
        任意の正方行列 $A$ に対して、以下の式が成り立つとき、$\lambda$ を固有値、$v$ を対応する固有ベクトルと呼ぶ：

        $$A v = \lambda v$$

        固有値分解では、行列 $A$ を以下のように分解できる：

        $$A = P \Lambda P^{-1}$$

        ここで：
        - $P$ は固有ベクトルを列とする行列
        - $\Lambda$ は固有値を対角成分とする対角行列
        - $P^{-1}$ は $P$ の逆行列

        ただし、すべての行列が固有値分解できるわけではない。

        ### 特異値分解 (SVD: Singular Value Decomposition)
        一方、特異値分解はより正方行列ではない行列にも適用できる。$m \times n$ 行列 $A$ の特異値分解は：

        $$A = U \Sigma V^T$$

        ここで：
        - $U$ は $m \times m$ の直交行列（左特異ベクトル）
        - $\Sigma$ は $m \times n$ の対角行列（特異値を対角成分に持つ）
        - $V^T$ は $n \times n$ の直交行列の転置（右特異ベクトル）

        ## 両者の関係

        1. 行列 $A^T A$ の固有ベクトルは、$A$ の右特異ベクトル $V$ と一致する
        2. $A A^T$ の固有ベクトルは、$A$ の左特異ベクトル $U$ と一致する
        3. $A^T A$ および $A A^T$ の固有値は $\sigma_i^2$ となる（ここで $\sigma_i$ は $A$ の特異値）

        数式を見ると：

        $$A^T A = (U \Sigma V^T)^T (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T = V \Sigma^T \Sigma V^T = V \Sigma^2 V^T$$

        同様に：

        $$A A^T = U \Sigma V^T V \Sigma^T U^T = U \Sigma \Sigma^T U^T = U \Sigma^2 U^T$$

        これらは固有値分解の形式になっていて、$\Sigma^2$ が固有値を対角成分に持つ行列。このように、$A^T A$ の固有ベクトルが $V$ と一致し、$A A^T$ の固有ベクトルが $U$ と一致する。
        """
    )
    return


@app.cell
def _():
    return


if __name__ == "__main__":
    app.run()
